---
---


@article{galke2020incremental,
        title = {Incremental Training of Graph Neural Networks on Temporal Graphs under Distribution Shift},
        abstract = {Current graph neural networks (GNNs) are promising, especially when the entire graph is known for training. However, it is not yet clear how to efficiently train GNNs on temporal graphs, where new vertices, edges, and even classes appear over time. We face two challenges: First, shifts in the label distribution (including the appearance of new labels), which require adapting the model. Second, the growth of the graph, which makes it, at some point, infeasible to train over all vertices and edges. We address these issues by applying a sliding window technique, i.e., we incrementally train GNNs on limited window sizes and analyze their performance. For our experiments, we have compiled three new temporal graph datasets based on scientific publications and evaluate isotropic and anisotropic GNN architectures. Our results show that both GNN types provide good results even for a window size of just 1 time step. With window sizes of 3 to 4 time steps, GNNs achieve at least 95% accuracy compared to using the entire timeline of the graph. With window sizes of 6 or 8, at least 99% accuracy could be retained. These discoveries have direct consequences for training GNNs over temporal graphs.},
        author = {Galke, Lukas and Vagliano, Iacopo, and Scherp, Ansgar},
        booktitle = {Preprint},
        arxiv = {2006.14422},
        code = {https://github.com/lgalke/Incremental-GNNs},
        year = {2020}
}

@inproceedings{galke2019graph,
	abstract = {Large-scale graph data in real-world applications is often not static but dynamic, i. e., new nodes and edges appear over time. Current graph convolution approaches are promising, especially, when all the graph{\rq}s nodes and edges are available during training. When unseen nodes and edges are inserted after training, it is not yet evaluated whether up-training or re-training from scratch is preferable. We construct an experimental setup, in which we insert previously unseen nodes and edges after training and conduct a limited amount of inference epochs. In this setup, we compare adapting pretrained graph neural networks against retraining from scratch. Our results show that pretrained models yield high accuracy scores on the unseen nodes and that pretraining is preferable over retraining from scratch. Our experiments represent a first step to evaluate and develop truly online variants of graph neural networks.},
	arxiv = {1905.06018},
	author = {Galke, Lukas and Vagliano, Iacopo and Scherp, Ansgar},
	booktitle = {Representation Learning on Graphs and Manifolds, {ICLR} Workshop},
	code = {https://github.com/lgalke/gnn-pretraining-evaluation},
	pdf = {2019-RLGM-online-graph-neural-nets.pdf},
	poster = {2019-RLGM-online-graph-neural-nets.poster.pdf},
	title = {Can Graph Neural Networks Go ``Online''? An Analysis of Pretraining and Inference},
	year = {2019}
}

@inproceedings{mai2019cbow,
	abstract = {Continuous Bag of Words (CBOW) is a powerful text embedding method.
            Due to its strong capabilities to encode word content, CBOW
            embeddings perform well on a wide range of downstream tasks while
            being efficient to compute. However, CBOW is not capable of
            capturing the word order. The reason is that the computation of
            CBOW's word embeddings is commutative, i.e., embeddings of XYZ and
            ZYX are the same. In order to address this shortcoming, we propose
            a learning algorithm for the Continuous Matrix Space Model, which
            we call Continual Multiplication of Words (CMOW). Our algorithm is
            an adaptation of word2vec, so that it can be trained on large
            quantities of unlabeled text. We empirically show that CMOW better
            captures linguistic properties, but it is inferior to CBOW in
            memorizing word content. Motivated by these findings, we propose a
            hybrid model that combines the strengths of CBOW and CMOW. Our
            results show that the hybrid CBOW-CMOW-model retains CBOW's strong
            ability to memorize word content while at the same time
            substantially improving its ability to encode other linguistic
            information by 8\%. As a result, the hybrid also performs better on
            8 out of 11 supervised downstream tasks with an average
            improvement of 1.2\%.},
	arxiv = {1902.06423},
	author = {Mai, Florian and Galke, Lukas and Scherp, Ansgar},
	booktitle = {International Conference on Learning Representations},
	code = {https://github.com/florianmai/word2mat},
	pdf = {2019-ICLR-word2mat.pdf},
	title = {{CBOW} Is Not All You Need: Combining {CBOW} with the Compositional Matrix Space Model},
	url = {https://openreview.net/forum?id=H1MgjoR9tQ},
	year = {2019}
}

@inproceedings{10.1007/978-3-030-04257-8_30,
	abstract = {While there are many studies on information retrieval models
              using full-text, there are presently no comparison studies of
              full-text retrieval vs. retrieval only over the titles of
              documents. On the one hand, the full-text of documents like
              scientific papers is not always available due to, e.g., copyright
              policies of academic publishers. On the other hand, conducting a
              search based on titles alone has strong limitations. Titles are
              short and therefore may not contain enough information to yield
              satisfactory search results. In this paper, we compare different
              retrieval models regarding their search performance on the
              full-text vs. only titles of documents. We use different
              datasets, including the three digital library datasets: EconBiz,
              IREON, and PubMed. The results show that it is possible to build
              effective title-based retrieval models that provide competitive
              results comparable to full-text retrieval. The difference between
              the average evaluation results of the best title-based retrieval
              models is only 3{\%} less than those of the best full-text-based
              retrieval models.},
	address = {Cham},
	author = {Saleh, Ahmed and Beck, Tilman and Galke, Lukas and Scherp, Ansgar},
	booktitle = {Maturity and Innovation in Digital Libraries},
	isbn = {978-3-030-04257-8},
	pages = {290--303},
	publisher = {Springer International Publishing},
	slides = {2018-ICADL-fulltext-vs-title-IR.slides.pdf},
	title = {Performance Comparison of Ad-Hoc Retrieval Models over Full-Text vs. Titles of Documents},
	year = {2018}
}

@inproceedings{Vagliano:2018:UAA:3267471.3267476,
	abstract = {The task of automatic playlist continuation is generating a list
              of recommended tracks that can be added to an existing playlist.
              By suggesting appropriate tracks, i.\,e., songs to add to a
              playlist, a recommender system can increase the user engagement
              by making playlist creation easier, as well as extending
              listening beyond the end of current playlist. The ACM Recommender
              Systems Challenge 2018 focuses on such task. Spotify released a
              dataset of playlists, which includes a large number of playlists
              and associated track listings. Given a set of playlists from
              which a number of tracks have been withheld, the goal is
              predicting the missing tracks in those playlists. We participated
              in the challenge as the team \emph{Unconscious Bias} and, in this
              paper, we present our approach. We extend adversarial
              autoencoders to the problem of automatic playlist continuation.
              We show how multiple input modalities, such as the playlist
              titles as well as track titles, artists and albums, can be
              incorporated in the playlist continuation task.},
	acmid = {3267476},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Vagliano, Iacopo and Galke, Lukas and Mai, Florian and Scherp, Ansgar},
	booktitle = {Proceedings of the ACM Recommender Systems Challenge},
	code = {https://github.com/lgalke/aae-recommender},
	doi = {10.1145/3267471.3267476},
	isbn = {978-1-4503-6586-4},
	keywords = {adversarial autoencoders, automatic playlist continuation, multi-modal recommender, music recommender systems, neural networks},
	location = {Vancouver, BC, Canada},
	numpages = {6},
	pages = {5:1--5:6},
	publisher = {ACM},
	series = {RecSys Challenge '18},
	slides = {2018-RecSysChallenge-aae-recommender.slides.pdf},
	title = {Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation},
	url = {http://doi.acm.org/10.1145/3267471.3267476},
	year = {2018}
}

@inproceedings{10.1007/978-3-319-99133-7_18,
	abstract = {We analyze the problem of response suggestion in a closed domain
              along a real-world scenario of a digital library. We present a
              text-processing pipeline to generate question-answer pairs from
              chat transcripts. On this limited amount of training data, we
              compare retrieval-based, conditioned-generation, and dedicated
              representation learning approaches for response suggestion. Our
              results show that retrieval-based methods that strive to find
              similar, known contexts are preferable over parametric approaches
              from the conditioned-generation family, when the training data is
              limited. We, however, identify a specific representation learning
              approach that is competitive to the retrieval-based approaches
              despite the training data limitation.},
	address = {Cham},
	author = {Galke, Lukas and Gerstenkorn, Gunnar and Scherp, Ansgar},
	booktitle = {Database and Expert Systems Applications},
	code = {https://github.com/lgalke/resuggest},
	isbn = {978-3-319-99133-7},
	pages = {218--229},
	pdf = {2018-TIR-response-suggestion.pdf},
	publisher = {Springer International Publishing},
	slides = {2018-TIR-response-suggestion.slides.pdf},
	title = {A Case Study of Closed-Domain Response Suggestion with Limited Training Data},
	year = {2018}
}

@inproceedings{Galke:2018:MAA:3209219.3209236,
	abstract = {We present multi-modal adversarial autoencoders for
              recommendation and evaluate them on two different tasks: citation
              recommendation and subject label recommendation. We analyze the
              effects of adversarial regularization, sparsity, and different
              input modalities. By conducting 408 experiments, we show that
              adversarial regularization consistently improves the performance
              of autoencoders for recommendation. We demonstrate, however, that
              the two tasks differ in the semantics of item co-occurrence in
              the sense that item co-occurrence resembles relatedness in case
              of citations, yet implies diversity in case of subject labels.
              Our results reveal that supplying the partial item set as input
              is only helpful, when item co-occurrence resembles relatedness.
              When facing a new recommendation task it is therefore crucial to
              consider the semantics of item co-occurrence for the choice of an
              appropriate model.},
	acmid = {3209236},
	address = {New York, NY, USA},
	author = {Galke, Lukas and Mai, Florian and Vagliano, Iacopo and Scherp, Ansgar},
	booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
	code = {https://github.com/lgalke/aae-recommender},
	doi = {10.1145/3209219.3209236},
	isbn = {978-1-4503-5589-6},
	keywords = {adversarial autoencoders, multi-modal, neural networks, recommender systems, sparsity},
	location = {Singapore, Singapore},
	numpages = {9},
	pages = {197--205},
	pdf = {2018-UMAP-aae-recommender.pdf},
	publisher = {ACM},
	series = {UMAP '18},
	slides = {2018-UMAP-aae-recommender.slides.pdf},
	title = {Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels},
	url = {http://doi.acm.org/10.1145/3209219.3209236},
	year = {2018}
}

@inproceedings{Lauscher:2018:LOC:3197026.3197050,
	abstract = {Citations play a crucial role in the scientific discourse, in
              information retrieval, and in bibliometrics. Many initiatives are
              currently promoting the idea of having free and open citation
              data. Creation of citation data, however, is not part of the
              cataloging workflow in libraries nowadays. In this paper, we
              present our project Linked Open Citation Database, in which we
              design distributed processes and a system infrastructure based on
              linked data technology. The goal is to show that efficiently
              cataloging citations in libraries using a semi-automatic approach
              is possible. We specifically describe the current state of the
              workflow and its implementation. We show that we could
              significantly improve the automatic reference extraction that is
              crucial for the subsequent data curation. We further give
              insights on the curation and linking process and provide
              evaluation results that not only direct the further development
              of the project, but also allow us to discuss its overall
              feasibility.},
	acmid = {3197050},
	address = {New York, NY, USA},
	author = {Lauscher, Anne and Eckert, Kai and Galke, Lukas and Scherp, Ansgar and Rizvi, Syed Tahseen Raza and Ahmed, Sheraz and Dengel, Andreas and Zumstein, Philipp and Klein, Annette},
	booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
	doi = {10.1145/3197026.3197050},
	isbn = {978-1-4503-5178-2},
	keywords = {automatic reference extraction, citation data, editorial system, library workflows, linked open data},
	location = {Fort Worth, Texas, USA},
	numpages = {10},
	pages = {109--118},
	publisher = {ACM},
	series = {JCDL '18},
	title = {Linked Open Citation Database: Enabling Libraries to Contribute to an Open and Interconnected Citation Graph},
	url = {http://doi.acm.org/10.1145/3197026.3197050},
        code = {https://github.com/locdb/},
	year = {2018}
}

@inproceedings{Mai:2018:UDL:3197026.3197039,
	abstract = {For (semi-)automated subject indexing systems in digital
              libraries, it is often more practical to use metadata such as the
              title of a publication instead of the full-text or the abstract.
              Therefore, it is desirable to have good text mining and text
              classification algorithms that operate well already on the title
              of a publication. So far, the classification performance on
              titles is not competitive with the performance on the full-texts
              if the same number of training samples is used for training.
              However, it is much easier to obtain title data in large
              quantities and to use it for training than full-text data. In
              this paper, we investigate the question how models obtained from
              training on increasing amounts of title training data compare to
              models from training on a constant number of full-texts. We
              evaluate this question on a large-scale dataset from the medical
              domain (PubMed) and from economics (EconBiz). In these datasets,
              the titles and annotations of millions of publications are
              available, and they outnumber the available full-texts by a
              factor of 20 and 15, respectively. To exploit these large amounts
              of data to their full potential, we develop three strong deep
              learning classifiers and evaluate their performance on the two
              datasets. The results are promising. On the EconBiz dataset, all
              three classifiers outperform their full-text counterparts by a
              large margin. The best title-based classifier outperforms the
              best full-text method by 9.9\%. On the PubMed dataset, the best
              title-based method almost reaches the performance of the best
              full-text classifier, with a difference of only 2.9\%.},
	acmid = {3197039},
	address = {New York, NY, USA},
	author = {Mai, Florian and Galke, Lukas and Scherp, Ansgar},
	booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
	code = {https://github.com/florianmai/Quadflor},
	doi = {10.1145/3197026.3197039},
	isbn = {978-1-4503-5178-2},
	keywords = {deep learning, digital libraries, text classification},
	location = {Fort Worth, Texas, USA},
	numpages = {10},
	pages = {169--178},
	publisher = {ACM},
	series = {JCDL '18},
	slides = {2018-JCDL-multi-label-classification.slides.pdf},
	title = {Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text},
	url = {http://doi.acm.org/10.1145/3197026.3197039},
	year = {2018}
}

@inproceedings{Galke:2017:UTV:3148011.3148039,
	abstract = {We conduct the first systematic comparison of automated semantic
              annotation based on either the full-text or only on the title
              metadata of documents. Apart from the prominent text
              classification baselines kNN and SVM, we also compare recent
              techniques of Learning to Rank and neural networks and revisit
              the traditional methods logistic regression, Rocchio, and Naive
              Bayes. Across three of our four datasets, the performance of the
              classifications using only titles reaches over 90\% of the quality
              compared to the performance when using the full-text.},
	acmid = {3148039},
	address = {New York, NY, USA},
	articleno = {20},
	arxiv = {1705.05311},
	author = {Galke, Lukas and Mai, Florian and Schelten, Alan and Brunsch, Dennis and Scherp, Ansgar},
	booktitle = {Proceedings of the Knowledge Capture Conference},
	code = {https://github.com/quadflor/Quadflor},
	doi = {10.1145/3148011.3148039},
	isbn = {978-1-4503-5553-7},
	keywords = {Multi-label classification, document analysis, semantic annotation},
	location = {Austin, TX, USA},
	numpages = {4},
	pages = {20:1--20:4},
	pdf = {2017-KCAP-quadflor.pdf},
	publisher = {ACM},
	series = {K-CAP 2017},
	slides = {2017-KCAP-quadflor.slides.pdf},
	title = {Using Titles vs. Full-text As Source for Automated Semantic Document Annotation},
	url = {http://doi.acm.org/10.1145/3148011.3148039},
	year = {2017}
}

@inproceedings{mci/Galke2017,
	abstract = {We assess the suitability of word embeddings for practical
              information retrieval scenarios. Thus, we assume that users issue
              ad-hoc short queries where we return the first twenty retrieved
              documents after applying a boolean matching operation between the
              query and the documents. We compare the performance of several
              techniques that leverage word embeddings in the retrieval models
              to compute the similarity between the query and the documents,
              namely word centroid similarity, paragraph vectors, Word Mover{\rq}s
              distance, as well as our novel inverse document frequency (IDF)
              re-weighted word centroid similarity. We evaluate the performance
              using the ranking metrics mean average precision, mean reciprocal
              rank, and normalized discounted cumulative gain. Additionally, we
              inspect the retrieval models{\rq} sensitivity to document length by
              using either only the title or the full-text of the documents for
              the retrieval task. We conclude that word centroid similarity is
              the best competitor to state-of-the-art retrieval models. It can
              be further improved by re-weighting the word frequencies with IDF
              before aggregating the respective word vectors of the embedding.
              The proposed cosine similarity of IDF re-weighted word vectors is
              competitive to the TF-IDF baseline and even outperforms it in
              case of the news domain with a relative percentage of 15\%.},
	address = {},
	author = {Galke, Lukas AND Saleh, Ahmed AND Scherp, Ansgar},
	booktitle = {INFORMATIK},
	code = {https://github.com/lgalke/vec4ir},
	pages = { 2155--2167 },
	pdf = {2017-INFORMATIK-vec4ir.pdf},
	publisher = {Gesellschaft f{\"u}r Informatik, Bonn},
	slides = {2017-INFORMATIK-vec4ir.slides.pdf},
	title = {Word Embeddings for Practical Information Retrieval},
	year = {2017}
}

@inproceedings{galke2019inductive,
	abstract = {
                    Automated research analyses are becoming more and more important as the volume of
                    research items grows at an increasing pace. 
                    We pursue a new direction for dynamic research analyses with graph neural
                    networks.
                    So far, graph neural networks have only been applied to small-scale datasets and
                    primarily supervised tasks such as node classification.
                    We propose to use an unsupervised training objective for concept representation learning
                    that is tailored towards bibliographic data with millions of research papers and
                    thousands of concepts from a controlled vocabulary.
                    We have evaluated the learned representations in clustering and classification
                    downstream tasks. Furthermore, we have conducted nearest concept queries in the
                    representation space.
                    Our results show that the representations learned by graph
                    convolution with our training objective are comparable to the ones learned by
                    the DeepWalk algorithm.
                    Our findings suggest that concept embeddings can be solely derived from the text
                    of associated documents without using a lookup-table embedding.
                    Thus, graph neural networks can operate on
                    arbitrary document collections without re-training. This property makes graph
                    neural networks useful for dynamic research analysis, which is often
                    conducted on time-based snapshots of bibliographic data.},
	address = {},
	author = {Galke, Lukas and Melnychuk, Tetyana and Seidlmayer, Eva and Trog, Steffen and Foerstner, Konrad U. and Schultz, Carsten and Tochtermann, Klaus},
	booktitle = {INFORMATIK},
	code = {https://github.com/lgalke/INFORMATIK2019-concept-representation-learning},
	publisher = {Gesellschaft f{\"u}r Informatik, Bonn},
	title = {Inductive Learning of Concept Representations from Library-Scale Corpora with Graph Convolution},
	year = {2019},
        pdf = {2019-INFORMATK-concept-representation-learning.pdf},
        slides = {2019-INFORMATK-concept-representation-learning.slides.pdf }
}

@mastersthesis{lgalke-mastersthesis,
	abstract = {
            We assess the suitability of word embeddings for practical information
            retrieval. While limiting ourselves to unsupervised models, we compare the
            performance of several techniques that leverage word embeddings to retrieval
            models (i.\ e.\@,  provide a query-document similarity): the intuitive word
            centroid similarity, dedicated paragraph vectors, the physically inspired Word
            Mover's distance, as well as a novel IDF re-weighted word centroid similarity.

            In our comparison, we thrive to simulate a strictly practical setting: short
            queries, a boolean matching operation, only the first twenty retrieved
            documents are considered. We evaluate the performance using the ranking metrics
            mean average precision, mean reciprocal rank and normalised discounted
            cumulative gain. Additionally, we inspect the retrieval models' sensitivity to
            document length by using either only the title or the full-text as documents.

            We conclude that word centroid similarity is the best competitor to
            state-of-the-art retrieval models and can be further improved by re-weighting
            the word frequencies according to inverse document frequency before aggregating
            the respective word vectors of the embedding. The proposed cosine similarity of
            IDF re-weighted word vectors is competitive to the TF-IDF baseline and even
            outperforms it in case of the news domain with a relative percentage of $15$\%.

            In the context of this research contribution, a dedicated information retrieval
            framework has been developed. The key features include the incorporation of
            embedding-based retrieval models, the simulation of a practical setting,
            automatic evaluation as well as convenient extendability by new retrieval
            models. The corresponding user's guide and developer's guide are part of this
            work.

            },
	author = {Galke, Lukas},
	code = {https://github.com/lgalke/vec4ir},
	month = 3,
	pdf = {lgalke-mastersthesis.pdf},
	school = {Kiel University, Germany},
	title = {Embedded Retrieval -- Word Embeddings for Practical Information Retrieval},
	year = 2017
}

@inproceedings{galke2019encoded,
	abstract = {We summarize our contribution to the International Conference on Learning Representations CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model, 2019. We construct a text encoder that learns matrix representations of words from unlabeled text, while using matrix multiplication as composition function. We show that our text encoder outperforms continuous bag-of-word representations on 9 out of 10 linguistic probing tasks and argue that the learned representations are complementary to the ones of vector-based approaches. Hence, we construct a hybrid model that jointly learns a matrix and a vector for each word. This hybrid model yields higher scores than purely vector-based approaches on 10 out of 16 downstream tasks in a controlled experiment with the same capacity and training data. Across all 16 tasks, the hybrid model achieves an average improvement of 1.2\%. These results are insofar promising, as they open up new opportunities to efficiently incorporate order awareness into word embedding models.},
	author = {Galke, Lukas and Mai, Florian and Scherp, Ansgar},
	booktitle = {INFORMATIK},
	publisher = {Gesellschaft f{\"u}r Informatik, Bonn},
	title = {What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function?},
	year = {2019},
        pdf = {2019-INFORMATIK-word2mat-ext-abs.pdf},
        slides = {2019-INFORMATIK-word2mat-ext-abs.slides.pdf}
}

